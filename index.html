<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>NetSci: NetSci: A Toolkit for High Performance Scientific Network Analysis Computation</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  extensions: ["tex2jax.js"],
  jax: ["input/TeX","output/HTML-CSS"],
});
</script>
<script type="text/javascript" async="async" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">NetSci
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">NetSci: A Toolkit for High Performance Scientific <a class="el" href="classNetwork.html">Network</a> Analysis Computation </div></div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#overview_sec">Overview</a></li>
<li class="level1"><a href="#install_sec">Installation</a></li>
<li class="level1"><a href="#theory">Theory</a><ul><li class="level2"><a href="#introduction">Introduction</a></li>
<li class="level2"><a href="#mutual_information">Mutual Information</a></li>
<li class="level2"><a href="#generalized_correlation">Generalized Correlation</a></li>
</ul>
</li>
<li class="level1"><a href="#Algorithms">Algorithms</a><ul><li class="level2"><a href="#mutual_information_gpu">Parallel Mutual Information</a><ul><li class="level3"><a href="#step1">Step 1: Initialization</a></li>
<li class="level3"><a href="#step2">Step 2: Parallel Processing</a></li>
<li class="level3"><a href="#step3">Step 3: Load Data in Chunks</a></li>
<li class="level3"><a href="#step4">Step 4: Find k Nearest Neighbors</a></li>
<li class="level3"><a href="#step5">Step 5: Increment Neighbor Counts</a></li>
<li class="level3"><a href="#step6">Step 6: Update Global Counts</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><h1><a class="anchor" id="overview_sec"></a>
Overview</h1>
<hr  />
<p>NetSci is a specialized toolkit designed for advanced network analysis in computational sciences. Utilizing the capabilities of modern GPUs, it offers a powerful and efficient solution for processing computationally demanding network analysis metrics while delivering state-of-the-art performance.</p>
<h1><a class="anchor" id="install_sec"></a>
Installation</h1>
<hr  />
<p>NetSci is designed with a focus on ease of installation and long-term stability, ensuring compatibility with Linux systems featuring CUDA-capable GPUs (compute capability 3.5 and above). It leverages well-supported core C++ and Python libraries to maintain simplicity and reliability.</p>
<ol type="1">
<li><b>Download Miniconda Installation Script</b>: <div class="fragment"><div class="line">wget https:<span class="comment">//repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh</span></div>
</div><!-- fragment --></li>
<li><b>Execute the Installation Script</b>: <div class="fragment"><div class="line">bash Miniconda3-latest-Linux-x86_64.sh</div>
</div><!-- fragment --></li>
<li><b>Update Environment Settings</b>: <div class="fragment"><div class="line">source ~/.bashrc</div>
</div><!-- fragment --></li>
<li><b>Install Git with Conda</b>: <div class="fragment"><div class="line">conda install -c conda-forge git</div>
</div><!-- fragment --></li>
<li><b>Clone the NetSci Repository</b>: <div class="fragment"><div class="line">git clone https:<span class="comment">//github.com/netscianalysis/netsci.git</span></div>
</div><!-- fragment --></li>
<li><b>Navigate to the NetSci Root Directory</b>: <div class="fragment"><div class="line">cd netsci</div>
</div><!-- fragment --></li>
<li><b>Create NetSci Conda Environment</b>: <div class="fragment"><div class="line">conda env create -f netsci.yml</div>
</div><!-- fragment --></li>
<li><b>Activate NetSci Conda Environment</b>: <div class="fragment"><div class="line">conda activate netsci</div>
</div><!-- fragment --></li>
<li><b>Create CMake Build Directory</b>: <div class="fragment"><div class="line">mkdir build</div>
</div><!-- fragment --></li>
<li><b>Set NetSci Root Directory Variable</b>: <div class="fragment"><div class="line">NETSCI_ROOT=$(pwd)</div>
</div><!-- fragment --></li>
<li><b>Navigate to the CMake Build Directory</b>: <div class="fragment"><div class="line">cd ${NETSCI_ROOT}/build</div>
</div><!-- fragment --></li>
<li><b>Compile CUDA Architecture Script</b>: <div class="fragment"><div class="line">nvcc ${NETSCI_ROOT}/build_scripts/cuda_architecture.cu -o cuda_architecture</div>
</div><!-- fragment --></li>
<li><b>Set CUDA Architecture Variable</b>: <div class="fragment"><div class="line">CUDA_ARCHITECTURE=$(./cuda_architecture)</div>
</div><!-- fragment --></li>
<li><b>Configure the Build with CMake</b>: <div class="fragment"><div class="line">cmake .. -DCONDA_DIR=$CONDA_PREFIX -DCUDA_ARCHITECTURE=${CUDA_ARCHITECTURE}</div>
</div><!-- fragment --></li>
<li><b>Build NetSci</b>: <div class="fragment"><div class="line">cmake --build . -j</div>
</div><!-- fragment --></li>
<li><b>Build NetSci Python Interface</b>: <div class="fragment"><div class="line">make python</div>
</div><!-- fragment --></li>
<li><b>Test C++ and CUDA Backend</b>: <div class="fragment"><div class="line">ctest</div>
</div><!-- fragment --></li>
<li><b>Run Python Interface Tests</b>: <div class="fragment"><div class="line">cd ${NETSCI_ROOT}</div>
<div class="line">pytest</div>
</div><!-- fragment --></li>
</ol>
<hr  />
<h1><a class="anchor" id="theory"></a>
Theory</h1>
<h2><a class="anchor" id="introduction"></a>
Introduction</h2>
<p>In the realm of network analysis, understanding the intricate relationships and dependencies between elements in a dataset is crucial. The 'netsci' library, developed using CUDA for GPU-accelerated performance, provides a sophisticated toolkit for such analysis. At the heart of this library lies a deep integration of core theoretical concepts from information theory and statistical analysis, tailored to extract meaningful insights from complex network data. This library harnesses the computational power of modern GPUs to perform high-speed calculations, enabling the analysis of large-scale networks that are often encountered in fields like social network analysis, biological network analysis, and communication networks. The underlying theory, rooted in principles of mutual information, Shannon entropy, and generalized correlation, provides a robust framework for understanding the dynamics and structure of networks. The utilization of CUDA not only accelerates computations but also allows for handling intricate calculations involving high-dimensional data, making 'netsci' an ideal choice for researchers and practitioners dealing with complex network systems. The integration of these theoretical concepts with state-of-the-art computational techniques opens new avenues for network analysis, offering insights that were previously challenging to obtain due to computational constraints. In the following sections, we delve into the core theoretical constructs that form the foundation of the 'netsci' library. Starting with Mutual Information, we explore how this measure serves as a cornerstone in understanding and quantifying the relationships between variables in a network. The subsequent sections will further elucidate how these theoretical principles are translated into practical, high-performance tools for network analysis.</p>
<h2><a class="anchor" id="mutual_information"></a>
Mutual Information</h2>
<p>Mutual information serves as a way to measure correlation, both linear and non-linear, between two random variables. Given a bivariate set of data \(z_i=(x_i,y_i),i=1,...,N\), which can be of any dimension, we assume that each of the \(N\) elements of the data are independent and identically distributed realizations of the random variables \(Z=(X,Y)\), and that they are distributed according to \(\mu(x,y)\), a proper smooth function. The marginal densities are \(\mu(x)=\int \mu(x,y)dy\) and \(\mu(y)=\int \mu(x,y)dx\).</p>
<p>The Shannon entropy can be defined as  </p><p class="formulaDsp">
\[
   H(X)=-\int\mu(x)\log\mu(x)dx
   \]
</p>
<p>where the base of the logarithm depends on the units desired for the information, whether bits ( \(\log_2\)), nats ( \(\log_e\)), decimal digits ( \(\log_{10}\)), or otherwise. In this work, we use the natural logarithm. The mutual information \(I(X,Y)\) is defined as:</p>
<p class="formulaDsp">
\[
   I(X,Y)=H(X)+H(Y)-H(X,Y)
   \]
</p>
<p>The value of \(I(X,Y)\) measures the strength of the connection between the variables \(X\) and \(Y\); if the two variables were completely independent, then \(I(X,Y)\) would be zero.</p>
<p>In most cases, the density distribution \(\mu\) is not known exactly, and must be estimated. Under the condition that \(\mu\) is a uniform distribution, we may approximate \(H(X)\) by a discrete sum,  </p><p class="formulaDsp">
\[
      \widehat{H}(X) = -\frac{1}{N}\sum_{i=1}^N\widehat{\log(\mu(x_i))}
   \]
</p>
<p> Now, an estimate for \(\widehat{\log(\mu(x_i))}\) must be defined. In this paper, we use a k-nearest neighbor estimator, which generalizes well to high-dimensions. In order to rank neighbors of a data point \(z_i\) by nearness, we use the max norm,  </p><p class="formulaDsp">
\[
   ||z-z&#39;|| = \max \{ ||x-x&#39;||,||y-y&#39;||\}
   \]
</p>
<p> where we choose to use a similar max norm for \(||x-x&#39;||\) and \(||y-y&#39;||\), although this is not required - a Euclidean norm could be used, for instance. For each data point \(z_i\), let \(\epsilon_x(i)/2\) and \(\epsilon_y(i)/2\) represent the distances from \(z_i\) to its \(k\)th nearest neighbor projected onto the X and Y subspaces, respectively. The value \(p_i\) is the integrated density within a distance \(\epsilon/2\) of the point \(x_i\), \(p_i(\epsilon)=\int_{||\xi-x_i||&lt;\epsilon/2}\mu(\xi)
   d\xi\). Consider the probability distribution  </p><p class="formulaDsp">
\[
   P_k(\epsilon_x, \epsilon_y)=P_k^{(b)}(\epsilon_x, \epsilon_y)
   +P_k^{(c)}(\epsilon_x, \epsilon_y)
   \]
</p>
<p> Specifically, \( P_k^{(b)}(\epsilon_x, \epsilon_y) \) represents the probability distribution that there are \( k-1 \) data points within the rectangle \(
   x_i\pm\epsilon_x(i)/2 \) and \( y_i\pm\epsilon_y(i)/2 \), a rectangle defined by the \( k \)th nearest neighbor within in the \( x
   \) subspace, \( N-k-1 \) points that are outside a different rectangle defined by \( x_i\pm(\epsilon_x(i)
   +d\epsilon_x)/2 \) and \( y_i\pm(\epsilon_y(i)+d\epsilon_y)/2 \), and one data point in the space between the two rectangles. The probability distribution \( P_k^{(c)}(\epsilon_x, \epsilon_y)
   \) is similar, though the rectangles are defined by the \( k \)th nearest neighbor within the \( y
   \) subspace, which may be the same, or a different, point used to define \( P_k^{(b)}(\epsilon_x, \epsilon_y) \). These quantities are then  </p><p class="formulaDsp">
\[
      P_k^{(b)}(\epsilon_x, \epsilon_y) = \begin{pmatrix}
      N - 1 \\
      k
      \end{pmatrix}\left(\frac{d^2[q_i^k]}{d\epsilon_x
      d\epsilon_y}\right)\left(1-p_i\right)^{N-1-k}
   \]
</p>
<p> and  </p><p class="formulaDsp">
\[
      P_k^{(c)}(\epsilon_x, \epsilon_y) = (k-1)\begin{pmatrix}
      N - 1 \\
      k
      \end{pmatrix}\left(\frac{d^2[q_i^k]}{d\epsilon_x
      d\epsilon_y}\right)\left(1-p_i\right)^{N-1-k}
   \]
</p>
<p> Similar to \( p_i \), the value \( q_i(\epsilon_x, \epsilon_y)
   \) is the integrated density within a tiny rectangle of size \( \epsilon_x \times \epsilon_y
   \) centered at \( (x_i, y_i) \). As mentioned before, \( p_i \) is the integrated density within a tiny square of side length \( \epsilon \) - tiny enough that we may assume that \( \mu(x) \) is constant within:  </p><p class="formulaDsp">
\[
   p_i(\epsilon)\approx c_d \epsilon^d \mu (x_i)
   \]
</p>
<p> where \( d \) is the dimension of \( x \), and \( c_d \) is the volume of the \( d \)-dimensional unit ball. For the maximum norm used in this study, we simply use \( c_d=1
   \). In this case,  </p><p class="formulaDsp">
\[
   I(X_1,X_2) = \psi(k) - 1/k - \langle \psi(n_x) + \psi(n_y) \rangle
   + \psi(N)
   \]
</p>
<p> where \( \psi(x) \) is the digamma function, and \( n_x(i) \) and \( n_y(i) \) are the number of points with distance less than or equal to \( \epsilon_x(i)/2 \) and \( \epsilon_y(i)/2 \), respectively. </p>
<h2><a class="anchor" id="generalized_correlation"></a>
Generalized Correlation</h2>
<p>The Pearson product-moment correlation is defined as  </p><p class="formulaDsp">
\[
      r(X, Y) = \frac{(X - \mu_{X})(Y - \mu_{Y})}{\sigma_{X}\sigma_{Y}},
   \]
</p>
<p> where \( \mu_{X} \) is the average of random variable \( X \), \( \mu_{Y} \) is the mean of random variable \( Y \), \( \sigma_{X} \) is the standard deviation of \( X \) and \( \sigma_{Y} \) is the standard deviation of \( Y \). The Pearson correlation defines the best linear fit between data sets sampled from \( X \) and \( Y \). As mentioned before, the Pearson correlation suffers a number of insufficiencies when used for data that is related nonlinearly, and sets of related data that oscillate in non-parallel directions. The Mutual Information (MI) can address these shortcomings. In order to provide an equivalent quantity to \( r \) in eq. pearson_correlation}, we define the generalized correlation coefficient \( r_{\textrm{MI}} \):  </p><p class="formulaDsp">
\[
      r_{\textrm{MI}}(X, Y) = \left(1-e^{-\frac{2I(X,Y)}{d}}\right)
      ^{\frac{1}{2}}
   \]
</p>
<p> Where \( d \) is the dimensionality of the data.</p>
<hr  />
<h1><a class="anchor" id="Algorithms"></a>
Algorithms</h1>
<h2><a class="anchor" id="mutual_information_gpu"></a>
Parallel Mutual Information</h2>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Variable   </th><th class="markdownTableHeadNone">Description    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><code>Xa</code>   </td><td class="markdownTableBodyNone">Array containing the data points for the first random variable in the mutual information calculation.    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><code>Xb</code>   </td><td class="markdownTableBodyNone">Array containing the data points for the second random variable in the mutual information calculation.    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><code>k</code>   </td><td class="markdownTableBodyNone">Integer specifying the number of nearest neighbors to consider for each data point.    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><code>n</code>   </td><td class="markdownTableBodyNone">Integer representing the total number of data points in each of the random variables <code>Xa</code> and <code>Xb</code>.    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><code>nXa</code>   </td><td class="markdownTableBodyNone">Array for storing the count of data points in <code>Xa</code> within a radius of epsilon_Xa / 2 for each point.    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><code>nXb</code>   </td><td class="markdownTableBodyNone">Array for storing the count of data points in <code>Xb</code> within a radius of epsilon_Xb / 2 for each point.    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><code>s_argMin</code>   </td><td class="markdownTableBodyNone">Shared memory array used to store the indices of the nearest neighbors during the calculation.    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><code>s_min</code>   </td><td class="markdownTableBodyNone">Shared memory array used to store the minimum distances calculated in the search for nearest neighbors.    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><code>s_epsXa</code>   </td><td class="markdownTableBodyNone">Shared memory array to store the maximum distance (epsilon) within <code>Xa</code> for the k-th nearest neighbors.    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><code>s_epsXb</code>   </td><td class="markdownTableBodyNone">Shared memory array to store the maximum distance (epsilon) within <code>Xb</code> for the k-th nearest neighbors.    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><code>s_nXa</code>   </td><td class="markdownTableBodyNone">Shared memory array used to temporarily store neighbor counts for <code>Xa</code> within each CUDA block.    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><code>s_nXb</code>   </td><td class="markdownTableBodyNone">Shared memory array used to temporarily store neighbor counts for <code>Xb</code> within each CUDA block.   </td></tr>
</table>
<h3><a class="anchor" id="step1"></a>
Step 1: Initialization</h3>
<ul>
<li><b>Input:</b> Arrays <code>Xa</code>, <code>Xb</code>, integers <code>k</code>, <code>n</code>, output arrays <code>nXa</code>, <code>nXb</code> </li>
<li><b>Output:</b> Updated <code>nXa</code>, <code>nXb</code> with neighbor counts for each point in <code>Xa</code>, <code>Xb</code> </li>
<li>Initialize shared memory arrays: <code>s_argMin</code>[1024], <code>s_min</code>[1024], <code>s_epsXa</code>[1], <code>s_epsXb</code>[1], <code>s_nXa</code>[1024], <code>s_nXb</code>[1024]</li>
</ul>
<h3><a class="anchor" id="step2"></a>
Step 2: Parallel Processing</h3>
<p>For each data point <code>i</code> processed in parallel CUDA blocks:</p><ul>
<li>Load <code>Xa</code>[i] and <code>Xb</code>[i] into registers <code>r_Xai</code>, <code>r_Xbi</code> </li>
<li>Set local thread index <code>localThreadIndex</code> = threadIdx.x</li>
<li>Initialize <code>s_nXa</code>[localThreadIndex], <code>s_nXb</code>[localThreadIndex] to zero</li>
<li>If <code>localThreadIndex</code> == 0, set <code>s_epsXa</code>[0], <code>s_epsXb</code>[0] to zero</li>
</ul>
<h3><a class="anchor" id="step3"></a>
Step 3: Load Data in Chunks</h3>
<ul>
<li>Iterate over <code>Xa</code>, <code>Xb</code> in chunks, loading into <code>r_Xa</code>, <code>r_Xb</code> </li>
<li>Synchronize threads using <code>__syncthreads()</code> </li>
</ul>
<h3><a class="anchor" id="step4"></a>
Step 4: Find k Nearest Neighbors</h3>
<ul>
<li>Initialize <code>localMin</code> = RAND_MAX, <code>localArgMin</code> = 0</li>
<li>Iterate over chunks, updating <code>localMin</code>, <code>localArgMin</code> based on distance <code>dX</code> between <code>r_Xai</code>, <code>r_Xbi</code> and chunk data</li>
<li>Update shared memory <code>s_min</code>, <code>s_argMin</code> </li>
<li>Perform parallel reduction to find global minimum distance and corresponding index</li>
<li>Update <code>s_epsXa</code>, <code>s_epsXb</code> and mark processed points in <code>r_Xa</code>, <code>r_Xb</code> as needed</li>
<li>Synchronize threads using <code>__syncthreads()</code> </li>
</ul>
<h3><a class="anchor" id="step5"></a>
Step 5: Increment Neighbor Counts</h3>
<ul>
<li>Iterate over chunks, incrementing <code>s_nXa</code>, <code>s_nXb</code> based on distance conditions to <code>r_Xai</code>, <code>r_Xbi</code> </li>
<li>Synchronize threads using <code>__syncthreads()</code> </li>
<li>Perform parallel reduction on <code>s_nXa</code>, <code>s_nXb</code> </li>
</ul>
<h3><a class="anchor" id="step6"></a>
Step 6: Update Global Counts</h3>
<ul>
<li>If <code>localThreadIndex</code> == 0, update <code>nXa</code>[i], <code>nXb</code>[i] with reduced counts from <code>s_nXa</code>[0], <code>s_nXb</code>[0]</li>
<li>Synchronize threads using <code>__syncthreads()</code> </li>
</ul>
</div></div><!-- PageDoc -->
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8
</small></address>
</body>
</html>
